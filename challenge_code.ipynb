{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d519415f-6a1e-4d04-918f-6f0eb0c96399",
   "metadata": {
    "name": "_0_HEADLINE",
    "collapsed": false
   },
   "source": "# Titanic - Machine Learning from Disaster\nSource:  \nhttps://www.kaggle.com/competitions/titanic\n\n### The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (i.e. name, age, gender, socio-economic class, etc).\n\n### What Data Will I Use in This Competition?\nIn this competition, you have access to a Snowflake table called **PASSENGERS**.  \nThis table contains the **PASSENGER_ID** and a label-column **SURVIVED** which indicates whether a passenger sruvived or not.  \nWe know the survival status for 891 passengers but the status for the remaining 418 passengers is unknown and therefore missing in that table.\n\nIn addition, your team of smart datascientists already registered a couple of features in your **Snowflake Feature Store**.  \nThis feature store includes information about the name, age, gender, socio-economic class, etc. \n\n### Evaluation\n#### Goal\nIt is your job to predict if a passenger survived the sinking of the Titanic or not.\nFor each passenger where the survival status is unknown, you must predict a 0 (died) or 1 (survived).\n\n#### Metric\nYour score is the percentage of passengers you correctly predict. This is known as accuracy.\n\n#### Submission\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.\nThe file should have exactly 2 columns:\n\nPassengerId (sorted in any order)\nSurvived (contains your binary predictions: 1 for survived, 0 for deceased)"
  },
  {
   "cell_type": "code",
   "id": "f0ff5630-5d7e-4028-b82a-d44dce4a2b13",
   "metadata": {
    "language": "python",
    "name": "_0_LIST_FILES",
    "collapsed": false
   },
   "outputs": [],
   "source": "# List available files (should also show Kaggle challenge files)\nfrom os import listdir\nlistdir()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1409aa3d-3e66-43e9-b832-61253bcc14ee",
   "metadata": {
    "name": "_1_IMPORTS1",
    "collapsed": false
   },
   "source": "# Imports"
  },
  {
   "cell_type": "code",
   "id": "4049a079-6d27-4d75-bb4d-ea763ae20052",
   "metadata": {
    "language": "python",
    "name": "_1_IMPORTS2",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Snowpark Imports\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.context import get_active_session\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.functions import col, lit, when\nfrom snowflake.snowpark.exceptions import SnowparkSessionException\nfrom snowflake.snowpark.functions import sproc\nfrom snowflake.snowpark import types as T\n\n# Snowpark ML\nfrom snowflake.ml.modeling.impute import SimpleImputer\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder, OneHotEncoder, Normalizer\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.modeling.metrics import accuracy_score\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\nfrom snowflake.cortex import Complete\n\n# Snowflake Task API\nfrom snowflake.core import Root\nfrom snowflake.core.database import Database\nfrom snowflake.core.schema import Schema\nfrom snowflake.core.warehouse import Warehouse\nfrom snowflake.core import Root\nfrom snowflake.core.task import StoredProcedureCall\nfrom snowflake.core.task.dagv1 import DAG, DAGTask, DAGOperation\nfrom snowflake.core._common import CreateMode\n\n# Other Imports\nfrom datetime import timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#import seaborn as sns\nimport json\nimport streamlit as st\n#import plotly.express as px\nfrom helper_functions import convert_column_name\nimport plotly.express as px\nfrom pprint import pprint\n\n#\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "333c75c1-0fa8-4bf0-a954-5c21237cd5d4",
   "metadata": {
    "name": "_2_SETUP1",
    "collapsed": false
   },
   "source": "# 2 - Set Up Environment"
  },
  {
   "cell_type": "code",
   "id": "496c9046-0510-44d5-96ef-03c354504ee2",
   "metadata": {
    "language": "python",
    "name": "cell18",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Retrieve the Session\nsession = get_active_session()\n\n# Set context\nsession.use_schema('KAGGLE_TITANIC_CHALLENGE.DEVELOPMENT')\n\n# Connect to Feature Store\nfs = FeatureStore(\n    session=session, \n    database=\"KAGGLE_TITANIC_CHALLENGE\", \n    name=\"DEVELOPMENT\", \n    default_warehouse=\"COMPUTE_WH\",\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "058c878f-f2eb-43bf-8a82-961a6b9a744e",
   "metadata": {
    "name": "_3_EXPLORATION1",
    "collapsed": false
   },
   "source": "# 3 - Data Exploration"
  },
  {
   "cell_type": "code",
   "id": "6dfc4be5-5932-4aa1-9596-e255060e0037",
   "metadata": {
    "language": "python",
    "name": "cell12",
    "collapsed": false
   },
   "outputs": [],
   "source": "passengers = session.table('PASSENGER_LABELS')\npassengers.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "218a3f56-17a7-4669-9f86-06def3366ddd",
   "metadata": {
    "language": "python",
    "name": "cell10",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Discover available entites, feature views and features\nprint('Entities:')\nfs.list_entities().show()\nentity = fs.get_entity(name=\"PASSENGER\")\n\nprint('Feature Views:')\nfs.list_feature_views().show()\nkaggle_fv = fs.get_feature_view('PASSENGER_KAGGLE_FEATURES','V1')\n\nprint('Features in PASSENGER_KAGGLE_FEATURES:')\npd.DataFrame(list(kaggle_fv.feature_descs.items()), columns=['FEATURE_NAME', 'DESCRIPTION'])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "06890236-8044-4d9a-9fc8-fa70252203a5",
   "metadata": {
    "language": "python",
    "name": "_3_VIEW_EXISTING_FEATURES",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Retrieve existing features for your data\ntitanic_df = fs.retrieve_feature_values(passengers, [kaggle_fv])\ntitanic_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a6e540c-8b37-45e0-a22c-2fa144a23ef1",
   "metadata": {
    "language": "python",
    "name": "_3_DESCRIBE1",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Statistics for all columns in dataset (where SURVIVED is not missing)\ntrain_df = titanic_df\ntrain_df_summary = train_df.describe().to_pandas()\ntrain_df_summary",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "506ddd51-77bd-4fe3-9d48-1e90b50b6eb5",
   "metadata": {
    "name": "_3_LLM_SUMMARY1",
    "collapsed": false
   },
   "source": "That's a ton of information! What do these statistics mean? What should I do?  \nLet's ask an LLM!"
  },
  {
   "cell_type": "code",
   "id": "bb20962f-f54f-47b0-986d-c15025cf0dbc",
   "metadata": {
    "language": "python",
    "name": "_3_LLM_SUMMARY2",
    "collapsed": false
   },
   "outputs": [],
   "source": "#llm = 'mixtral-8x7b'\nllm = 'llama3-70b'\n\nprompt = f\"\"\"\nI used Snowparks describe function to calculate count, mean, stddev, min and max per column.\nPASSENGER_ID is unique and identifies each row. I want to predict the variable SURVIVED.\nWhat feature engineering steps should I perform before building a machine learning model?\nMake sure to explain your recommendations based on the data.\n{train_df_summary.to_markdown()}\n\"\"\"\n\nresponse = Complete(llm, prompt)\nst.markdown(response)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5ee6fce7-c760-46ef-bc41-a3f497ab6339",
   "metadata": {
    "language": "python",
    "name": "_3_FEAT_SCALING_OUTLIER",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Create a figure and a set of subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n\n# Create a boxplot in the first subplot\nax1.boxplot(train_df[['AGE']].dropna().to_pandas())\nax1.set_title('Boxplot of AGE')\nax1.set_ylabel('Values')\n\n# Create a boxplot in the second subplot\nax2.boxplot(train_df[['FARE']].dropna().to_pandas())\nax2.set_title('Boxplot of FARE')\nax2.set_ylabel('Values')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56b58db8-fe6b-41f7-af03-d83ba870d79d",
   "metadata": {
    "language": "python",
    "name": "_3_CAT_BARPLOTS",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Visualize variables in relation to survival probability\ncol1, col2 = st.columns(2)\ncol1.bar_chart(train_df.group_by('SEX').agg(F.avg('SURVIVED').as_('SURVIVAL_PROB')).to_pandas(), x='SEX', y='SURVIVAL_PROB')\ncol2.bar_chart(train_df.group_by('EMBARKED').agg(F.avg('SURVIVED').as_('SURVIVAL_PROB')).to_pandas(), x='EMBARKED', y='SURVIVAL_PROB')\ncol1.bar_chart(train_df.group_by('PCLASS').agg(F.avg('SURVIVED').as_('SURVIVAL_PROB')).to_pandas(), x='PCLASS', y='SURVIVAL_PROB')\ncol2.bar_chart(train_df.group_by('SIB_SP').agg(F.avg('SURVIVED').as_('SURVIVAL_PROB')).to_pandas(), x='SIB_SP', y='SURVIVAL_PROB')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "84adf20b-219c-48a0-b476-227da4d5e0a8",
   "metadata": {
    "name": "_4_FEATURE_ENGINEERING",
    "collapsed": false
   },
   "source": "# 4 - Feature Engineering"
  },
  {
   "cell_type": "code",
   "id": "1acb44ff-89e9-4d65-9fcd-f957d73b4cc8",
   "metadata": {
    "language": "python",
    "name": "_4_CONVENIENCE_FEATURES",
    "collapsed": false
   },
   "outputs": [],
   "source": "# We generate two new variables for convenience:\n# Uppercase the SEX column (will be useful when we one-hot-encode this variable)\ntitanic_df = titanic_df.with_column('SEX', F.upper(col('SEX')))\n\n# Add the names for embarkation and uppercase them\n# Southampton (S), Cherbourg (C), Queenstown (Q)\ntitanic_df = titanic_df.with_column('EMBARKED', \n                                when(col('EMBARKED')=='S', lit('Southampton'))\n                                .when(col('EMBARKED')=='C', lit('Cherbourg'))\n                                .when(col('EMBARKED')=='Q', lit('Queenstown')))\ntitanic_df = titanic_df.with_column('EMBARKED', F.upper(col('EMBARKED')))\n\ntitanic_df.show(3)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0a759f98-1d8f-4438-8e17-0bdb11d11613",
   "metadata": {
    "language": "python",
    "name": "_4_FAMILY_SIZE",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Features: Family Size\n# We can imagine that large families will have more difficulties to evacuate, looking for theirs sisters/brothers/parents \n# during the evacuation. \n# So, i choosed to create a \"FAMILY_SIZE\" feature which is the sum of SIBSP , PARCH and 1 (including the passenger).\ntitanic_df = titanic_df.with_column('FAM_SIZE', col('SIB_SP') + col('PARCH') + 1)\n\n# We further can create groups based on the FAMILY_SIZE\ntitanic_df = titanic_df.with_column('FAM_SIZE_CATEGORY', \n                                when(col('FAM_SIZE') == 1, 'SINGLE')\n                                .when(col('FAM_SIZE') == 2, 'COUPLE')\n                                .when(col('FAM_SIZE') >= 5, 'LARGE_FAMILY')\n                                .otherwise('NORMAL_FAMILY'))\n\ntitanic_df[['FAM_SIZE','FAM_SIZE_CATEGORY','SURVIVED']].show(3)\n\n# Analyze family sizes for training data\nanalysis_df = titanic_df.filter(col('SURVIVED').is_not_null()).group_by('FAM_SIZE_CATEGORY')\nanalysis_df = analysis_df.agg(F.count('FAM_SIZE_CATEGORY').as_('COUNT'), F.avg('SURVIVED').as_('SURVIVAL_PROB')).order_by('SURVIVAL_PROB')\nanalysis_df = analysis_df.to_pandas()\n\nfig = px.scatter(analysis_df, x=\"COUNT\", y=\"SURVIVAL_PROB\", size=\"COUNT\", color=\"FAM_SIZE_CATEGORY\",\n                 hover_name=\"FAM_SIZE_CATEGORY\", title=\"Frequency of Family Sizes and Their Relation to Survival Probability\",\n                 labels={\"COUNT\": \"Count of Titles\", \"SURVIVAL_PROB\": \"Survival Probability\"},\n                 size_max=60)\n\nst.plotly_chart(fig, use_container_width=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "19fef19d-c7a3-431e-90af-bc23c3e6febb",
   "metadata": {
    "language": "python",
    "name": "_4_TITLE",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Retrieve title from name\ntitanic_df = titanic_df.with_column('TITLE',F.trim(F.split(F.split(col('NAME'), F.lit(','))[1],F.lit('.'))[0]))\ntitanic_df[['NAME','TITLE','SURVIVED']].show(3)\n\n# Combine rare titles\nrare_titles = ['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona','Ms','Mme','Mlle']\ntitanic_df = titanic_df.with_column('TITLE', when(col('TITLE').isin(rare_titles), 'Rare').otherwise(col('TITLE')))\n\n# Analyze titles\nanalysis_df = titanic_df.filter(col('SURVIVED').is_not_null()).group_by('TITLE')\nanalysis_df = analysis_df.agg(F.count('TITLE').as_('COUNT'), F.avg('SURVIVED').as_('SURVIVAL_PROB')).order_by('SURVIVAL_PROB')\nanalysis_df = analysis_df.to_pandas()\n\nfig = px.scatter(analysis_df, x=\"COUNT\", y=\"SURVIVAL_PROB\", size=\"COUNT\", color=\"TITLE\",\n                 hover_name=\"TITLE\", title=\"Frequency of Titles and Their Relation to Survival Probability\",\n                 labels={\"COUNT\": \"Count of Titles\", \"SURVIVAL_PROB\": \"Survival Probability\"},\n                 size_max=60)\n\nst.plotly_chart(fig, use_container_width=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a37112e-c8a3-4a73-b17d-1b837f50f64e",
   "metadata": {
    "language": "python",
    "name": "_4_LLM_FEATURE_DESC",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "llm = 'llama3-70b'\n\nprompt = f\"\"\"\nYou are provided with a SQL Query that derives features from existing columns.\nDescribe the features FAM_SIZE, FAM_SIZE_CATEGORY and TITLE.\nThe descriptions will be stored in a feature store, so make sure to return a JSON where the feature name is the key and the description is the value.\n{titanic_df.queries['queries'][0]}\n\"\"\"\n\nllm_response = Complete(llm, prompt)\nfeature_descriptions = json.loads(llm_response.split('```')[1])\nfor key in feature_descriptions:\n    feature_descriptions[key] = feature_descriptions[key].replace(\"'\", '')\nfeature_descriptions",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "533e251d-0007-46ba-9e47-acacacc4ff5c",
   "metadata": {
    "language": "python",
    "name": "_4_FEAT_STORE_TITLE_FAM_SIZE",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Create Feature View with Custom Features\ncustom_fv = FeatureView(\n    name=\"PASSENGER_CUSTOM_FEATURES\", \n    entities=[entity],\n    feature_df=titanic_df[['PASSENGER_ID','TITLE','FAM_SIZE','FAM_SIZE_CATEGORY']].drop('SURVIVED'), \n    refresh_freq=\"1 minute\",\n    desc=\"Custom Passenger Features\")\n\n# Add descriptions for some features\ncustom_fv = custom_fv.attach_feature_desc(feature_descriptions)\n\ncustom_fv = fs.register_feature_view(\n    feature_view=custom_fv, \n    version=\"V1\", \n    block=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "659269fa-ba59-470d-aba2-79048d42c558",
   "metadata": {
    "language": "python",
    "name": "_4_TRAIN_TEST_SPLIT",
    "collapsed": false
   },
   "outputs": [],
   "source": "spine_df = session.table('PASSENGER_LABELS')\n\nspine_df_train = spine_df.filter(col('SURVIVED').is_not_null())\nprint(f'Train dataset has {spine_df_train.count()} passengers.')\nspine_df_train.show(3)\n\nspine_df_test = spine_df.filter(col('SURVIVED').is_null())\nprint(f'Test dataset has {spine_df_test.count()} passengers.')\nspine_df_test.show(3)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bfd3a110-54eb-4aa0-8bbf-281239ed919d",
   "metadata": {
    "language": "python",
    "name": "_4_GENERATE_TRAIN_DATASET",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Generate the training dataset by retrieving the features\ntraining_dataset = fs.generate_dataset(\n    name=\"TITANIC_TRAINING_DATASET\",\n    spine_df=spine_df_train,\n    features=[kaggle_fv,custom_fv],\n    spine_label_cols=[\"SURVIVED\"],\n    desc=\"Training Data to train model to predict whether a passenger survived.\"\n)\n\n# Retrieve a Snowpark DataFrame from the registered Dataset\ntraining_dataset_df = training_dataset.read.to_snowpark_dataframe().cache_result()\ntraining_dataset_df.show(3)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "137ff3dc-0429-47a9-a420-d6d9f8f45d6f",
   "metadata": {
    "name": "_5_MODELLING",
    "collapsed": false
   },
   "source": "# 5 - Modelling"
  },
  {
   "cell_type": "code",
   "id": "38a42bb9-ce39-4efa-a0e0-50891b7050af",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.use_warehouse('TRAIN_WH')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9accec4a-0df9-497e-aa69-899dcb28b37c",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "collapsed": false
   },
   "outputs": [],
   "source": "# DROP unused variables\n# PASSENGER_ID -> just an artificial ID with no predictive value\n# NAME -> people won't survive just because of their name and we extracted the title already\n# CABIN -> too many missing values\n# TICKET -> doesn't contain valuable information in its current form since it's a unique value per customer\ntraining_dataset_df = training_dataset_df.drop(['PASSENGER_ID','NAME','CABIN','TICKET'])\n\n# Impute Age by mean\nsi_age =  SimpleImputer(\n    input_cols=['AGE','FARE'], \n    output_cols=['AGE_IMP','FARE_IMP'],\n    strategy='mean',\n    drop_input_cols=True\n)\n\n# Normalize Fare and Age\nnorm = Normalizer(\n    input_cols=['AGE_IMP','FARE_IMP'],\n    output_cols=['AGE_IMP_NORM','FARE_IMP_NORM'],\n    drop_input_cols=True\n)\n\n# One-Hot-Encoding\nohe = OneHotEncoder(\n    input_cols=['SEX','EMBARKED','TITLE','FAM_SIZE_CATEGORY'], \n    output_cols=['SEX','EMBARKED','TITLE','FAM_SIZE_CATEGORY'],\n    drop_input_cols=True\n)\n\n# Define the XGBoost model (incl. Hyperparameter Tuning)\nlabel_cols = ['SURVIVED']\noutput_cols = ['SURVIVED_PREDICTION']\n\ngrid_search = GridSearchCV(\n    estimator=XGBClassifier(),\n    param_grid={\n        'n_estimators':[10, 50, 100],\n        'max_depth': [2,4,8],\n        'learning_rate':[.01, .03, .1],\n    },\n    n_jobs = -1,\n    scoring=\"accuracy\",\n    label_cols=label_cols,\n    output_cols=output_cols\n)\n\n# Build the pipeline\nmodel_pipeline = Pipeline(\n    steps=[\n        (\"IMPUTE\",si_age),\n        (\"NORMALIZE\",norm),\n        (\"ONE_HOT_ENCODE\",ohe),\n        (\"GRIDSEARCH_XGBOOST\",grid_search)\n    ]\n)\n\n# Fit the pipeline to the training data\nfitted_pipeline = model_pipeline.fit(training_dataset_df)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cabd2960-a911-47f2-9a3c-28ca5a29d477",
   "metadata": {
    "name": "_6_MODEL_EVALUATION",
    "collapsed": false
   },
   "source": "# 6 - Model Evaluation"
  },
  {
   "cell_type": "code",
   "id": "da094c3b-796a-4cf5-adb1-a1e653c29aed",
   "metadata": {
    "language": "python",
    "name": "cell16",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Build a dataframe for the gridsearch results\nmodel_object = fitted_pipeline.to_sklearn().named_steps['GRIDSEARCH_XGBOOST']\ngs_results = model_object.cv_results_\nn_estimators_val = []\nlearning_rate_val = []\nmax_depth_val = []\nfor param_dict in gs_results[\"params\"]:\n    n_estimators_val.append(param_dict[\"n_estimators\"])\n    learning_rate_val.append(param_dict[\"learning_rate\"])\n    max_depth_val.append(param_dict[\"max_depth\"])\naccuracy_val = gs_results[\"mean_test_score\"]\n\ngs_results_df = pd.DataFrame(data={\n    \"n_estimators\":n_estimators_val,\n    \"learning_rate\":learning_rate_val,\n    \"max_depth\":max_depth_val,\n    \"accuracy\":accuracy_val})\nprint(f'Number of Models: {len(gs_results_df)}')\nprint('Best Parameter Configuration:')\nmodel_object.best_params_\nprint(f'Accuracy of best Model: {model_object.best_score_}')\n\n# Create the 3D scatter plot to visualize impact of parameters on accuracy\nfig = px.scatter_3d(gs_results_df, x='learning_rate', y='n_estimators', z='max_depth', color='accuracy',\n                    labels={'accuracy': 'accuracy'})\n\n# Update the layout to increase the size of the chart\nfig.update_layout(\n    width=1000,  # Set the desired width\n    height=800   # Set the desired height\n)\n\n# Display the chart\nst.plotly_chart(fig)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c319b5d-9a1c-45bd-9b3c-a83ea0058e29",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Generate the training dataset by retrieving the features\ntest_dataset = fs.generate_dataset(\n    name=\"TITANIC_TEST_DATASET\",\n    spine_df=spine_df_test,\n    features=[kaggle_fv,custom_fv],\n    exclude_columns=['SURVIVED'],\n    desc=\"Test Data to evaluate model to predict whether a passenger survived.\"\n)\n\n# Retrieve a Snowpark DataFrame from the registered Dataset\ntest_dataset_df = test_dataset.read.to_snowpark_dataframe().cache_result()\ntest_dataset_df.show(3)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "702ee5f1-7999-46dd-bd91-0895505a53ce",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "7d472789-d9be-4adf-9b04-9da138cd792e",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Create reference to model registry\nmodel_registry = Registry(\n    session=session, \n    database_name=session.get_current_database(), \n    schema_name=session.get_current_schema()\n)\n\n# Register new model version\nregistered_model = model_registry.log_model(\n    fitted_pipeline,\n    model_name=\"TITANIC_SURVIVAL_MODEL\",\n    comment=\"Model trained using GridsearchCV in Snowpark to predict survival of Titanic passengers.\",\n    metrics={\"accuracy\": model_object.best_score_},\n    conda_dependencies=['xgboost']\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "78df827e-6a04-47ea-b6e7-a23a6d81eebf",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Create and persist predictions from registered model given the retrieved features\npredictions = registered_model.run(test_dataset_df, function_name='predict')\npredictions.write.save_as_table('TITANIC_TEST_PREDICTIONS', mode='overwrite')\n\n#\npredictions = session.table('TITANIC_TEST_PREDICTIONS')\npredictions.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4a90cfb-b7bd-419a-aea8-bd3e9cbf879b",
   "metadata": {
    "language": "python",
    "name": "cell9",
    "collapsed": false
   },
   "outputs": [],
   "source": "kaggle_submission = predictions.select('PASSENGER_ID','SURVIVED_PREDICTION')\nkaggle_submission = kaggle_submission.with_column_renamed(col('PASSENGER_ID'),'\"PassengerId\"')\nkaggle_submission = kaggle_submission.with_column_renamed(col('SURVIVED_PREDICTION'),'\"Survived\"')\nkaggle_submission.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db3b27df-b23e-40b0-a093-d712a0af4c3e",
   "metadata": {
    "language": "python",
    "name": "cell8",
    "collapsed": false
   },
   "outputs": [],
   "source": "kaggle_submission.write.csv(\n    '@KAGGLE_SUBMISSION/submission.csv', \n    header=True, \n    single=True, \n    format_type_options={\"COMPRESSION\": \"NONE\"},\n    overwrite=True\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "632d70ba-1f2c-4256-863a-b17159de45dc",
   "metadata": {
    "language": "python",
    "name": "cell24",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.call('calculate_challenge_score', '@KAGGLE_TITANIC_CHALLENGE.DEVELOPMENT.KAGGLE_SUBMISSION/submission.csv')",
   "execution_count": null
  }
 ]
}